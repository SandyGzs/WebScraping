from algoliasearch.search_client import SearchClient
import time
import pandas as pd
import os
import re
import requests
from DrissionPage import ChromiumPage, ChromiumOptions
from tqdm import tqdm
from fake_useragent import UserAgent
from concurrent.futures import ThreadPoolExecutor
from validators import url
import subprocess

# Initialize
client = SearchClient.create('P2DFUDCD21', '94dece75845ae66b39d67e442486da66')  # I am to lazy to update the read api key - you can always find it by directly calling website once a while.
index = client.init_index('wp_posts_post')
dfs = []

# 2011-01-01 TO 2024-04-01
for d in tqdm(pd.date_range(start=pd.to_datetime("2011-01-01"), end=pd.to_datetime("2024-04-01"), freq="ME")):
    st, ed = pd.to_datetime(f"{d.year}-{d.month}-01"), d
    st, ed = int(time.mktime(st.timetuple())), int(time.mktime(ed.timetuple()))
    page = 0
    result = index.search("展",
                          {
                              "page": page,
                              "hitsPerPage": 1000,
                              "filters": f"post_date:{st} TO {ed}"
                          }
                          )
    data = pd.DataFrame(result.get("hits"))
    dfs.append(data)
    while True:
        page += 1
        result = index.search("展",
                              {
                                  "page": page,
                                  "hitsPerPage": 1000,
                                  "filters": f"post_date:{st} TO {ed}"
                              }
                              )
        data = pd.DataFrame(result.get("hits"))
        if data.empty:
            break
        else:
            dfs.append(data)
df = pd.concat(dfs).drop_duplicates("post_id").reset_index()

# Open Info
chunk = 10  # 1...10
df = df.sort_values(by="post_date_formatted", ascending=False)[
     1000 * (chunk - 1):1000 * (chunk - 0)]  # 10k are chosen but load by each 1k chunk -- my server has a shallow storage so it needs more chunks
df = df[df["post_id"] != "1382934"]
permalink = df["permalink"].values.tolist()
post_id = df["post_id"].values.tolist()

# Prepare User Agents
ua = UserAgent(os="macos", min_version=120.0, browsers="chrome", platforms="pc")

# Create Main Folder
directory = f"{os.getcwd()}/main"
os.makedirs(directory, exist_ok=True)


# Main Function
def func(link, ID):

    # Create|Search Folder
    filepath = f"{os.getcwd()}\\main\\{ID}"
    in_process = False if os.path.isdir(filepath + "finished") else True
    if in_process:
        os.makedirs(filepath, exist_ok=True)

    # Chrome
    co = ChromiumOptions().auto_port()
    co.set_user_agent(user_agent=ua.random)
    co.headless()
    co.incognito()
    driver = ChromiumPage(co)
    if url(f"{link}?lang=cn") and in_process:
        in_process = driver.get(url=f"{link}?lang=cn")

    # Article Title
    article_title = "not found"
    if in_process:
        driver.wait.eles_loaded(".single-content")
        article_title = driver.s_ele(".single-content").text if driver.s_ele(".single-content") else "not found"

    # Author Info
    summary = {}
    author_url, author_name = ["not found"] * 2
    if in_process:
        driver.wait.eles_loaded(".spec")
        if driver.s_eles(".spec"):
            summary = {
                child.s_ele(".spec-label").text: [
                    child.s_ele(".spec-data").child().attr("href"),
                    " ".join([ele.text for ele in child.s_ele(".spec-data").children()])
                ] for child in driver.s_eles(".spec")
            }
            if "设计公司:" in summary:
                author_url, author_name = summary.get("设计公司:")

    # Article Content Text
    l = []
    if in_process:
        driver.wait.eles_loaded(".client-render")

        # Pictures + Titles
        if driver.s_ele(".client-render"):
            paragraphs = driver.s_ele(".client-render").child().children()
            for p in paragraphs:
                if p.attr("class") == "ss-twelvecol shortcolumn":
                    for x in p.children():
                        l.append(x)
                else:
                    l.append(p)

    # Create Article Document
    if in_process:
        with open(f'{filepath}\\article.txt', 'w', encoding="utf-8") as f:
            stop_words = ["，", ",", "；"]
            f.write(f"文章标题: {article_title}\n")
            f.write(f"文章链接: {link}\n")
            if summary != {}:
                for key, value in summary.items():
                    f.write(f"{key} {value[1]}\n")
            img_number = 0
            last_picture_index = 0

            if l:
                for j in range(len(l)):
                    if l[j].s_ele(".colorbox_gallery"):
                        last_picture_index = j

                for j in range(len(l)):
                    driver.wait.eles_loaded(".colorbox_gallery")
                    img_block = l[j].s_ele(".colorbox_gallery")
                    if img_block:
                        img_url1 = img_block.child().attr("src")
                        if url(img_url1):
                            file_format = img_url1.split(".")[-1]
                            img_url2 = f"""{"-".join(img_url1.split("-")[:-1])}.{file_format}""" if "-" in img_url1 and "x" in img_url1 else img_url1
                            filename = f"{filepath}\\{img_number}.{file_format}"
                            tab = driver.new_tab(url=img_url2)
                            header = tab.s_ele(".header")
                            img_data = requests.get(img_url1).content if header else requests.get(img_url2).content
                            with open(filename, 'wb') as handler:
                                handler.write(img_data)
                            f.write(f"图片插入: {img_number}.{file_format}\n")
                            img_number += 1
                            tab.close()
                    else:
                        if j < last_picture_index:
                            pattern = re.compile(r'[\u4e00-\u9fa5]')
                            sentences = l[j].text.split("©")[0]
                            photographer = "©" + l[j].text.split("©")[1] if "©" in l[j].text else ""
                            if pattern.search(sentences):
                                sentences = "\n".join([item for item in sentences.split("\n") if pattern.search(item)])
                                if "▼" in sentences or "▲" in sentences:
                                    for stop_word in stop_words:
                                        sentences = stop_word.join(
                                            [item for item in sentences.split(stop_word) if pattern.search(item)])
                                f.write(f"{sentences}{photographer}\n")
                        else:
                            f.write(f"{l[j].text}\n")

    # Author HomePage
    if url(author_url) and in_process:
        in_process = driver.get(author_url)

    if in_process:
        driver.wait.eles_loaded(".user-avatar")
        logo_url = driver.s_ele(".user-avatar").child().attr("src") if driver.s_ele(".user-avatar") else "not found"

        # Save Logo
        if url(logo_url):
            file_format = logo_url.split(".")[-1]
            img_data = requests.get(logo_url).content
            filename = f"{filepath}/logo.{file_format}"
            with open(filename, 'wb') as handler:
                handler.write(img_data)

        # Create Author Document
        with open(f'{filepath}\\author.txt', 'w', encoding="utf-8") as f:
            f.write(f"作者: {author_name}\n")
            location = driver.s_ele(".user-location").text if driver.s_ele(".user-location") else "not found"
            f.write(f"作者地址: {location}\n")
            summary = driver.s_ele(".user-about").text if driver.s_ele(".user-about") else "not found"
            f.write(f"作者简介: {summary}\n")
            if driver.s_ele(".user-sns"):
                for p in driver.s_ele(".user-sns").children():
                    if p.children():
                        if p.child(1).attr("class"):
                            contact_type = p.child(1).attr("class").lstrip("fa fa").lstrip("-")
                            chn_map = {
                                "link": "链接",
                                "phone": "手机",
                                "envelope": "信箱",
                                "facebook": "脸书",
                                "wechat": "微信",
                                "weibo": "微博",
                                "douban": "豆瓣",
                                "map-maker": "地址"
                            }
                            contact_type = chn_map.get(contact_type) if contact_type in chn_map else contact_type
                            f.write(f"{contact_type}: {p.text}\n")

    if not os.path.isdir(filepath + "finished"):
        os.rename(filepath, filepath + "finished")
    driver.quit()


# Process
with ThreadPoolExecutor(max_workers=8) as executor:
    stack = list(tqdm(executor.map(func, permalink, post_id), total=len(permalink)))
subprocess.call("TASKKILL /f  /IM  CHROME.EXE")  # delete the chromes running backend
